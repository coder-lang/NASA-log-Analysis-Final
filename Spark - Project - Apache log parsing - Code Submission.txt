# Here in the use case, we will look at analyzing Apache web logs from NASA using Python, Spark.
from pyspark.sql import Row
import re

# Expression pattern to extract fields from the log line
PATTERN = '^(\S+) (\S+) (\S+) \[([\w:/]+\s[+\-]\d{4})\] "(\S+) (\S+)(.*)" (\d{3}) (\S+)'

#Example line from NASA log
#line = 'uplherc.upl.com - - [01/Aug/1995:00:00:08 -0400] "GET /images/ksclogo-medium.gif HTTP/1.0" 304 0'

line = 'piweba4y.prodigy.com - - [01/Aug/1995:00:00:10 -0400] "GET /images/launchmedium.gif HTTP/1.0" 200 11853'

# Parse a line in the Apache Common Log format
def parseLogLine(log):
    m = re.match(PATTERN, log)
    if m:
        return [Row(host=m.group(1), timeStamp=m.group(4),url=m.group(6), httpCode=int(m.group(8)))]
    else:
        return []

#Check whether it is working
parseLogLine(line)



from pyspark.sql import SparkSession

#Building sparksession Set a name for the application, which will be shown in the Spark web UI.
#Gets an existing SparkSession or, if there is no existing one, creates a new one based on the options set in this builder.
spark = SparkSession.builder.appName("PythonWordCount").getOrCreate()
sc = spark.sparkContext

# File is available at /data/spark/project/NASA_access_log_Aug95.gz in Hue
logFile = sc.textFile("/data/spark/project/NASA_access_log_Aug95.gz")

#RDD 
accessLog = logFile.flatMap(parseLogLine)

#spark sql data frame
accessDf = spark.createDataFrame(accessLog)
accessDf.printSchema()

#Creates a new temporary view using a SparkDataFrame in the Spark Session. 
accessDf.createOrReplaceTempView("nasalog")
output = spark.sql("select * from nasalog")
output.createOrReplaceTempView("nasa_log")

Problem 1

Write spark code to find out top 10 requested URLs along with a count of the number of times they have been requested
(This information will help the company to find out most popular pages and how frequently they are accessed)

Command Code-
endpointCounts = (accessLog.map(lambda log: (log.url, 1)).reduceByKey(lambda a, b : a + b))
#This code is for fifteen we can change takeorder as per requirement
topEndpoints = endpointCounts.takeOrdered(15, lambda s: -1 * s[1])
# We can write Top Ten or Top Fifteen in print command as per requirement
print 'Top Ten URLs/Endpoints requested: %s' % topEndpoints

Answer-

Top Ten URLs/Endpoints requested: [(u'/images/NASA-logosmall.gif', 97410), (u'/images/KSC-logosmall.gif', 75337), (u'/images/MOSAIC-logosmall.gif', 67448
), (u'/images/USA-logosmall.gif', 67068), (u'/images/WORLD-logosmall.gif', 66444), (u'/images/ksclogo-medium.gif', 62778), (u'/ksc.html', 43687), (u'/his
tory/apollo/images/apollo-logo1.gif', 37826), (u'/images/launch-logo.gif', 35138), (u'/', 30347), (u'/images/ksclogosmall.gif', 27810), (u'/shuttle/missi
ons/sts-69/mission-sts-69.html', 24606), (u'/shuttle/countdown/', 24461), (u'/shuttle/missions/sts-69/count69.gif', 24383), (u'/shuttle/missions/sts-69/s
ts-69-patch-small.gif', 23405)]

Problem 2-

Write spark code to find out the top five-time frame for high traffic (which day of the week or hour of the day receives peak traffic,
this information will help the company to manage resources for handling peak traffic load)

Command Code-

spark.sql("select substr(timeStamp,1,14) as timeFrame,count(*) as req_cnt from nasa_log group by substr(timeStamp,1,14) order by req_cnt desc LIMIT 5").show()

Answer-

+--------------+-------+
|     timeFrame|req_cnt|
+--------------+-------+
|31/Aug/1995:11|   6319|
|31/Aug/1995:10|   6283|
|31/Aug/1995:13|   5948|
|30/Aug/1995:15|   5919|
|31/Aug/1995:09|   5627|
+--------------+-------+


Problem 3 -

Write spark code to find out top 5 time frames of least traffic 
(which day of the week or hour of the day receives least traffic, 
this information will help the company to do production deployment in that time frame 
so that less number of users will be affected if something goes wrong during deployment)

Command Code-

spark.sql("select substr(timeStamp,1,14) as timeFrame,count(*) as req_cnt from nasa_log group by substr(timeStamp,1,14) order by req_cnt  LIMIT 5").show()

Answer-

+--------------+-------+
|     timeFrame|req_cnt|
+--------------+-------+
|03/Aug/1995:04|     16|
|03/Aug/1995:09|     22|
|03/Aug/1995:05|     43|
|03/Aug/1995:10|     57|
|03/Aug/1995:07|     58|
+--------------+-------+


Problem 4 -

Write Spark code to find out unique HTTP codes returned by the server
along with count (this information is helpful for DevOps team to find out 
how many requests are failing so that appropriate action can be taken to fix the issue)

Command Code-

responseCodeToCount = (accessLog.map(lambda log: (log.httpCode, 1)).reduceByKey(lambda a, b : a + b).cache())                       
responseCodeToCountList = responseCodeToCount.takeOrdered(100)
print 'Found %d response codes' % len(responseCodeToCountList)
print 'Response Code Counts: %s' % responseCodeToCountList

Answer-
Response Code Counts: [(200, 1398988), (302, 26444), (304, 134146), (403, 171), (404, 10056), (500, 3), (501, 27)]


#####Project Completed######






